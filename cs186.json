[{"pageContent":"  Memory and Disk  Whenever a database uses data, that data must exist in memory. Accessing this data is relatively fast, but once the data becomes very large, it becomes impossible to fit all of it within memory. Disks are used to cheaply store all of a database’s data, but they incur a large cost whenever data is accessed or new data is written.","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Files, Pages, Records  The basic API for disks includes READ and WRITE which stands for transferring “pages” of data from disk to RAM and transferring “pages” of data from RAM to disk respectively. Note that both API calls are very slow due to the structure of magnetic disks.","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Choosing File Types  Platters usually spin at around 15000 rpm. The arm assembly moves in or out to position a head on a desired track which is under heads and makes a “cylinder”. Only one head reads/writes at any one time The block/page size is a multiple of (fixed) sector size.","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Heap File  ","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Sorted Files  Times to access (read/write) a disk block are listed as below:","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  A Note on Counting Header Pages  • seek time (moving arms to position disk head on track);  2-3 ms on average","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Record Types  • rotational delay (waiting for block to rotate under head); 0-4 ms (15000 RPM)","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Page Formats  • transfer time (actually moving data to/from disk surface); 0.25 ms per 64KB page","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Practice Questions  Solid State Drives (SSD), or Flash, is another medium to store data. Different from magnetic disks, SSDs are organized into cells, and supports fast random reads. Note that in contrast to SSDs, hard disks perform very bad on random reads because spatial locality is important for disks and sequential reads are much faster. SSDs support fine-grain reads (4-8K reads) and coarse-grain writes. However, the cells in SSDs tend to wear out after a certain number of erasures for writes (only 2k-3k erasures before failure). To combat this issue, it uses a technique called \"wear leveling\" to keep moving write units around to make sure that no single cell is repeatedly written over and over. SSDs are generally more expensive than hard disks too. (1-2MB writes)","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Solutions  ","metadata":{"source":"https://cs186berkeley.net/notes/note3/"}},{"pageContent":"  Introduction  In the previous notes, we went over different file and record representations for data storage. This week, we will introduce index which is a data structure that operates on top of the data files and helps speed up reads on a specific key. You can think of data files as the actual content of a book and a index as the table of content for fast lookup. We use indexes to make queries run faster, especially those that are run frequently. Consider a web application that looks up the record for a user in a Users table based on the username during the login process. An index on the username column will make login faster by quickly finding the row of the user trying to log in. In this course note, we will learn about B+ trees, which is a specific type of index. Here is an example of what a B+ tree looks like:","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Properties  ","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Insertion  The number d is the order of a B+ tree. Each node (with the exception of the root node) must have d ≤ x ≤ 2d entries assuming no deletes happen (it’s possible for leaf nodes to end up with < d entries if you delete data). The entries within each node must be sorted.","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Deletion  In between each entry of an inner node, there is a pointer to a child node. Since there are at most 2d entries in a node, inner nodes may have at most 2d+1 child pointers. This is also called the tree’s fanout.","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Storing Records  The keys in the children to the left of an entry must be less than the entry while the keys in the children to the right must be greater than or equal to the entry.","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Clustering  All leaves are at the same depth and have between d and 2d entries (i.e., at least half full)","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Counting IO’s  For example, here is a node of an order d=2 tree:","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Bulk Loading  ","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Past Exam Problems  Note that the node satisfies the order requirement, also known as the occupancy invariant (d ≤ x ≤ 2d) because d=2 and this node has 3 entries which satisfies 2 ≤ x ≤ 4.","metadata":{"source":"https://cs186berkeley.net/notes/note4/"}},{"pageContent":"  Introduction  So far, we have discussed how disk space is managed at the lowest level of the database management system and how files and indexes are managed in our page-based database system. We will now explore the interface between these two levels on the DBMS - the buffer manager.","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  Buffer Pool  ","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  Handling Page Requests   The buffer manager is responsible for managing pages in memory and processing page requests from the file and index manager. Remember, space on memory is limited, so we cannot afford to store all pages in the buffer pool. The buffer manager is responsible for the eviction policy, or choosing which pages to evict when space is filled up. When pages are evicted from memory or new pages are read in to memory, the buffer manager communicates with the disk space manager to perform the required disk operations.","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  LRU Replacement and Clock Policy  ","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  MRU Replacement  Memory is converted into a buffer pool by partitioning the space into frames that pages can be placed in. A buffer frame can hold the same amount of data as a page can (so a page fits perfectly into a frame). To efficiently track frames, the buffer manager allocates additional space in memory for a metadata table.","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  Practice Questions  ","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  Solutions   The table tracks 4 pieces of information:","metadata":{"source":"https://cs186berkeley.net/notes/note5/"}},{"pageContent":"  Motivation  In the previous notes we talked about how SQL is a declarative programming language. This means that you specify what you want, but you don’t have to specify how to do it. This is great from a user’s perspective as it makes the queries much easier to write. As database engineers, however, we often want a language that is more expressive. When we study query optimization in a few weeks we’re going to want a way to express the many different valid plans a database can use to execute a query. For this we will use Relational Algebra, a procedural programming language (meaning that the query specifies exactly what operators to use and in what order).","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Relational Algebra Introduction  All of the operators in relational algebra take in a relation and output a relation. A basic query looks like this:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Projection (\\(\\pi\\))  The \\(\\pi\\) operator picks only the columns that it wants to advance to the next operator (just like SQL SELECT). In this case, the operator takes the dogs relation in as a parameter and returns a relation that only has the name column of the dogs relation. An important fact about relational algebra is that the relations are sets of tuples, meaning that they cannot have duplicates in them. If the dogs relation is initially:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Selection (\\(\\sigma\\))  The query above would return:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Union (\\(\\cup\\))  Initially the two Busters are different because they have different ages, but once you get rid of the age column, they become duplicates, so only one remains in the output relation.  Let’s formally introduce the relational algebra operators.","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Set Difference (-)  We have already been introduced to the projection operator which takes in a single relation as input and selects only the columns specified. The columns are specified in the subscript of the operator like almost all parameters to operators. The output schema of projection is determined by the schema of the column list. The projection operator is relational algebra’s version of the SQL SELECT clause.  We now can express SQL queries involving just the SELECT and FROM clauses with relational algebra. For example the SQL query:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Intersection (\\(\\cap\\))  Can be represented with the expression we introduced in section 2:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Cross Product (\\(\\times\\))  Note that there is no operator equivalent to the FROM operator in relational algebra because the parameters of these operators specify which tables we pull from.","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Joins (\\(\\bowtie\\))  The selection operator takes in a single relation and filters rows based on a certain condition. The output schema will be the same as the input schema, and duplicate elimination is not needed for selection. Don’t let the name confuse you - this operator is equivalent to SQL’s WHERE clause, not its SELECT clause. Let’s try to express the following query in terms of relational algebra:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Rename (\\(\\rho\\))  The equivalent relational algebra expression is:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Group By / Aggregation (\\(\\gamma\\))  Another correct expression for that query is:","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Practice Questions  This illustrates the beauty of relational algebra. There is only one (reasonable) way to write SQL for what the query is trying to accomplish, but we can come up with multiple different expressions in relational algebra that get the same result. In the first expression we select only the columns we want first, and then we filter out the rows we don’t want. In the second we filter the rows first and then select the columns. We will soon learn ways to evaluate which of these plans is better!\\","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Solutions  The selection operator also supports compound predicates. The $\\wedge$ symbol corresponds to the AND keyword in SQL and the $\\vee$ symbol corresponds to the OR keyword. For example,","metadata":{"source":"https://cs186berkeley.net/notes/note6/"}},{"pageContent":"  Motivation  Sometimes, sorting is a bit overkill for the problem. Often, all we want is to group the same value together, but we do not actually care about the order the values appear in (think GROUP BY or de-duplication). In a database, grouping like values together is called hashing. We cannot build a hash table in the standard way you learned in 61B for the same reason we could not use quick sort in the last note: we cannot fit all of our data in memory! Let’s see how to build an efficient out-of-core hashing algorithm.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  General Strategy  Because we cannot fit all of the data in memory at once, we’ll need to build several different hash tables and concatenate them together. There is a problem with this idea though. What happens if we build two separate hash tables that each have the same value in them (e.g. “Brian” occurs in both tables)? Concatenating the the tables will result in some of the “Brian”s not being right next to each other.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  The Algorithm  To fix this, before building a hash table out of the data in memory, we need to guarantee that if a certain value is in memory, all of its occurrences are also in memory. In other words, if “Brian” occurs in memory at least once, then we can only build the hash table if every occurrence of “Brian” in our data is currently in memory. This ensures that values can only appear in one hash table, making the hash tables safe to concatenate.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  Drawbacks of Hashing  We will use a divide and conquer algorithm to solve this problem. During the “divide” phase we perform partitioning passes, and during the “conquer” phase we construct the hash tables. Just like in the sorting note, we will assume that we have \\(B\\) buffer frames available to us. As we hash our data, we will use \\(1\\) of these buffers as an input buffer, and \\(B-1\\) of these buffers as output buffers.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  Example  We begin by streaming data from disk into our \\(1\\) input buffer. Once we have the input data, we want to break it into partitions. A partition is a set of pages such that for a particular hash function, the values on the pages all hash to the same hash value. The first partitioning pass will hash each record in the input buffer into one of \\(B-1\\) partitions, with each partition corresponding to one of our \\(B-1\\) output buffers. If an output buffer fills up, that page is flushed to disk, and the hashing process continues. Any flushed pages that came from the same buffer will be located adjacent to each other on disk. At the end of the first partitioning pass, we have \\(B-1\\) partitions stored on disk, with each partition’s data stored continuously.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  Analysis of External Hashing  One important property of partitions is that if a certain value appears in one partition, all occurrences of that value in our data will be located in the same partition. This is because identical values will be hashed to the same value. For example, if “Brian” appears in one partition, “Brian” will not appear in any other partition.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  Practice Questions  After the first partitioning pass, we have \\(B-1\\) partitions of varying sizes. For partitions that can fit in memory (the partition’s size is less than or equal to \\(B\\) pages), we can go right into the “conquer” phase, and begin building hash tables. For the partitions that are too big, we need to recursively partition them using a different hash function than we used in the first pass. Why a different hash function? If we reused the original function, every value would hash to its original partition so the partitions would not get any smaller. We can recursively partition with different hash functions as many times as necessary until all of the partitions have at most B pages.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  Solutions  Once all of our partitions can fit in memory, and all of the same values occur in the same partition (by the property of partitions), we can begin the the “conquer” phase. First we select a new hash function for the purpose of building our final hash table. Then, for each partition, we stream the partition into memory, create a hash table using the new hash function, and flush the resulting hash table back to disk.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  Past Exam Questions  It is important to note that hashing is very susceptible to data skew. Data skew occurs when the values that we are hashing do not follow a uniform distribution. Due to the property of hashed partitions (that all hashes of the same value will end up in the same partition), data skew can lead to very unevenly-sized partitions, which is sub-optimal for our purposes. Additionally, not all hash functions are created equal. In the optimal case, our hash function is a uniform hash function, which partitions the data into \"uniform\", equal-sized partitions. However, hash functions are often non-uniform, and partition data unevenly across all of the partitions. For the purposes of this class, we will be using uniform hash functions unless otherwise stated. In the two images below, cylinders represent steps that happen on disk, squares represent steps that happen in memory, and the lines between the two represent data streaming from disk to memory and vice versa.","metadata":{"source":"https://cs186berkeley.net/notes/note7/"}},{"pageContent":"  I/O Review  In CS61B, you learned about many different sorting algorithms. Why are we learning yet another new one in this class? All of the traditional sorting algorithms (i.e. quick sort, insertion sort, etc.) rely on us being able to store all of the data in memory. This is a luxury we do not have when developing a database. In fact, most of the time our data will be an order of magnitude larger than the memory available to us.","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Two Way External Merge Sort  Remember that we incur 1 I/O any time we either write a page from memory to disk or read a page from disk into memory. Because of how time consuming it is to go to disk, we only look at the number of I/Os an algorithm incurs when analyzing its performance instead of traditional measures of algorithmic complexity like big-O. Therefore, when developing our sorting algorithm we will attempt to minimize the number of I/Os it will incur. When counting I/Os, we ignore any potential caching done by the buffer manager. This implies that once we unpin the page and say that we are done using it, the next time we attempt to access the page it will always cost 1 I/O. Think of this as the buffer manager evicting the data page from the buffer pool once the data page is no longer being used.","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Analysis of Two Way Merge  Let’s start by developing a sorting algorithm that works but is not as good as possible. Because we cannot keep all of our data in memory at one time, we know that we are going to sort different pieces of it separately and then merge it together.","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Full External Sort  In order to merge two lists together efficiently, they must be sorted first. This is a hint that the first step of our sorting algorithm should be to sort the records on each individual page. We’ll call this first phase the “conquer” phase because we are conquering individual pages.","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Analysis of Full External Merge Sort  After this, let’s start merging the pages together using the merge algorithm from merge sort. We’ll call the result of these merges sorted runs. A sorted run is a sequence of pages that is sorted.","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Practice Questions  The rest of the algorithm will simply be to continue merging these sorted runs until we have only one sorted run remaining. One sorted run implies that our data is fully sorted! See the image on the next page for a diagram of the algorithm run to completion.","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Solutions  ","metadata":{"source":"https://cs186berkeley.net/notes/note8/"}},{"pageContent":"  Introduction  Let’s begin with the simplest question: what, exactly, is a join? If you remember the SQL project, you’ll remember writing things like R INNER JOIN S ON R.name = S.name and other similar statements. What that actually meant is that you take two relations, R and S, and create one new relation out of their matches on the join condition – that is, for each record \\(r_i\\) in R, find all records \\(s_j\\) in S that match the join condition we have specified and write \\(<r_i, s_j>\\) as a new row in the output (all the fields of r followed by all the fields of s). The SQL lecture slides are a great resource for more clarifications on what joins actually are.1 Before we get into the different join algorithms, we need to discuss what happens when the new joined relation consisting of \\(<r_i, s_j>\\) is formed. Whenever we compute the cost of a join, we will ignore the cost of writing the joined relation to disk. This is because we are assuming that the output of the join will be consumed by another operator involved later on in the execution of the SQL query. Often times this operator can directly consume the joined records from memory so we don’t need to write the joined table to disk. Don’t worry if this sounds confusing right now; we will revisit it in the Query Optimization module, but the important thing to remember for now is that the final write cost is not included in our join cost models!","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Simple Nested Loop Join  Let’s start with the simplest strategy possible. Let’s say we have a buffer of B pages, and we wish to join two tables, R and S, on the join condition \\(\\theta\\). Starting with the most naïve strategy, we can take each record in R, search for all its matches in S, and then we yield each match. This is called simple nested loop join (SNLJ). You can think of it as two nested for loops: \\","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Page Nested Loop Join  for each record \\(r_i\\) in R:","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Block Nested Loop Join      for each record \\(s_j\\) in S:","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Visual Comparison of Loop Joins          if \\(\\theta\\)(\\(r_i\\),\\(s_j\\)):","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Index Nested Loop Join              yield <\\(r_i\\), \\(s_j\\)>","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Hash Join  ","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Sort-Merge Join  This would be a great thing to do, but the theme of the class is really centered around optimization and minimizing I/Os. For that, this is a pretty poor scheme, because we take each record in R and read in every single page in S searching for a match. The I/O cost of this would then be \\([R]+|R|[S]\\), where [R] is the number of pages in R and |R| is the number of records in R. And while we might be able to optimize things a slight amount by switching the order of R and S in the for loop, this really isn’t a very good strategy. Note: SNLJ does not incur |R| I/Os to read every record in R. It will cost [R] I/Os because it’s really doing something more like “for each page \\(p_r\\) in R: for each record \\(r\\) in \\(p_r\\): for each page \\(p_s\\) in S: for each record \\(s\\) in \\(p_s\\): join” since we can’t read less than a page at a time.","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Practice Questions  It’s clear that we don’t want to read in every single page of S for each record of R, so what can we do better? What if we read in every single page in S for every single page of R instead? That is, for a page of R, take all the records and match them against each record in S, and do this for every page of R. That’s called page nested loop join (PNLJ). Here’s the pseudocode for it:","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Solutions  for each page \\(p_r\\) in R:","metadata":{"source":"https://cs186berkeley.net/notes/note9/"}},{"pageContent":"  Introduction  When we covered SQL, we gave you a helpful mental model for how queries are executed. First you get all the rows in the FROM clause, then you filter out columns you don’t need in the WHERE clause, and so on. This was useful because it guarantees that you will get the correct result for the query, but it is not what databases actually do. Databases can change the order they execute the operations in order to get the best performance. Remember that in this class, we measure performance in terms of the number of I/Os. Query Optimization is all about finding the query plan that minimizes the number of I/Os it takes to execute the query. A query plan is just a sequence of operations that will get us the correct result for a query. We use relational algebra to express it. This is an example of a query plan:","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Iterator Interface  ","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Selectivity Estimation  First it joins the two tables together, then it filters out the rows, and finally it projects only the columns it wants. As we’ll soon see, we can come up with a much better query plan!","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Selectivity of Joins  The expression tree above is representative of some query plan, where the edges encode the “flow” of tuples, the vertices represent relational algebra operators, and the source vertex represents a table access operator. For some SQL query, there may be multiple ways to receive the same output; the goal of a query optimizer is to generate a sequence of operators that returns the correct output efficiently. Once this sequence is generated, the query executor then creates instances of each operator, each of which implements an iterator interface. The iterator interface is responsible for efficiently executing operator logic and forwarding relevant tuples to next operator. Every relational operator is implemented as a subclass of the Iterator class, so in executing a query plan, we will call next() on the root operator, and recurse down until the base case. For example, in the diagram below, the root operator (SELECT sname) will call next(), which will recursively call next() for the join iterator, which recursively calls next() down the query until we reach the base case. We call this a pull-based computation model. Each time we call next(), we will either perform a streaming (\"on-the-fly\") or blocking (\"batch\") algorithm on the operator instance. Streaming operators use a small amount of work to produce each tuple, like a SELECT statement. Blocking operators like sort operations, on the other hand, do not produce output until they consume their entire input.","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Common Heuristics  ","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Pass 1 of System R  In this way, we can see how query plans are single-threaded: we simply recursively call next() on the query plan until we reach our base operator.","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Passes 2..n  An important property of query optimization is that we have no way of knowing how many I/Os a plan will cost until we execute that plan. This has two important implications. The first is that it is impossible for us to guarantee that we will find the optimal query plan - we can only hope to find a good (enough) one using heuristics and estimations. The second is that we need some way to estimate how much a query plan costs. One tool that we will use to estimate a query plan’s cost is called selectivity estimation. The selectivity of an operator is an approximation for what percentage of pages will make it through the operator onto the operator above it. This is important because if we have an operator that greatly reduces the number of pages that advance to the next stage (like the WHERE clause), we probably want to do that as soon as possible so that the other operators have to work on fewer pages.","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Calculating I/O Costs for Join Operations  Most of the formulas for selectivity estimation are fairly straightforward. For example, to estimate the selectivity of a condition of the form \\(X=3\\), the formula is 1 / (number of unique values of X). The formulas used in this note are listed below, but please see the lecture/discussion slides for a complete list. The last section of this note also contains the important formulas. In these examples, capital letters are for columns and lowercase letters represent constants. All values in the following examples are integers, as floats have different formulas.","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Practice Questions  X=a: 1/(unique vals in X)","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Solutions  X=Y: 1/max(unique vals in X, unique vals in Y)","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Appendix - Selectivity Values  X\\(>\\)a: (max(X) - a) / (max(X) - min(X) + 1)","metadata":{"source":"https://cs186berkeley.net/notes/note10/"}},{"pageContent":"  Introduction  In most situations, we usually don’t have just one person accessing a database at a time. Many users can make requests to a database at the same time which can cause concurrency issues. What happens when one user writes and then another user reads from the same resource? What if both users try to write to the same resource? There are several problems we can run into when several users are using the database at the same time if we’re not careful:","metadata":{"source":"https://cs186berkeley.net/notes/note11/"}},{"pageContent":"  Transactions  Inconsistent Reads (Write-Read Conflict): A user reads only part of what was updated.","metadata":{"source":"https://cs186berkeley.net/notes/note11/"}},{"pageContent":"  Motivation for Concurrent Execution  User 1 updates Table 1 and then updates Table 2.","metadata":{"source":"https://cs186berkeley.net/notes/note11/"}},{"pageContent":"  Concurrency Control  User 2 reads Table 2 (which User 1 has not updated yet) and then Table 1 (which User 1 already updated) so it reads the database in an intermediate state.","metadata":{"source":"https://cs186berkeley.net/notes/note11/"}},{"pageContent":"  Conclusion  Lost Update (Write-Write Conflict): Two users try to update the same record at the same time so one of the updates gets lost. For example:","metadata":{"source":"https://cs186berkeley.net/notes/note11/"}},{"pageContent":"  Introduction  In the last note, we introduced the concept of isolation as one of the ACID properties. Let’s revisit our definition here:","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Two Phase Locking  This note will go into details on how the DBMS is able to interleave the actions of many transactions, while guaranteeing isolation.","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Lock Management  What are locks, and why are they useful? Locks are basically what allows a transaction to read and write data. For example, if Transaction \\(T1\\) is reading data from resource \\(A\\), then it needs to make sure no other transaction is modifying resource \\(A\\) at the same time. So a transaction that wants to read data will ask for a Shared (S) lock on the appropriate resource, and a transaction that wants to write data will ask for an Exclusive (X) lock on the appropriate resource. Only one transaction may hold an exclusive lock on a resource, but many transactions can hold a shared lock on data. Two phase locking (2PL) is a scheme that ensures the database uses conflict serializable schedules. The two rules for 2PL are:","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Deadlock  Transactions must acquire a S (shared) lock before reading, and an X (exclusive) lock before writing.","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Lock Granularity  Transactions cannot acquire new locks after releasing any locks – this is the key to enforcing serializability through locking!","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Practice Problems   ","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Solutions  Two Phase Locking guarantees conflict serializability. When a committing transaction has reached the end of its acquisition phase, lets call this the “lock point”. At this point, it has everything that it needs locked. Any conflicting transactions either started the release phase before this point or are blocked waiting for this transaction. So the visibility of actions of two conflicting transactions can be ordered by their lock points. The order of these lock points gives us an equivalent serial schedule!","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Appendix  The problem with this is that it does not prevent cascading aborts. For example,","metadata":{"source":"https://cs186berkeley.net/notes/note12/"}},{"pageContent":"  Introduction  So far, we’ve learned how to use already built databases: SQL queries! We also learned the internals of a database management system which includes many abstraction levels. But what happens if we are given a high-level description of what data we want to store with our database? How do we design a database to fit out needs? We will discuss these question in this module!","metadata":{"source":"https://cs186berkeley.net/notes/note13/"}},{"pageContent":"  Entity-Relationship Models  When designing a database, we often use Entity-Relationship Models (aka \"E-R\" models). These models have 2 main components. The first main component is an entity: a real-world object described by a set of attribute values. An entity is usually depicted as a rectangle in our E-R model. In addition, the attributes/fields associated with the entity are depicted as ovals. For example, the following figure shows an employee as an entity in our E-R model with the following fields: a SSN number, a name, and a lot. Note that the attribute \"ssn\" is underlined because it is the identifying attribute for this entity! In a schema, this would be the primary key.","metadata":{"source":"https://cs186berkeley.net/notes/note13/"}},{"pageContent":"  FDs and Normalization  ","metadata":{"source":"https://cs186berkeley.net/notes/note13/"}},{"pageContent":"  Practice Questions  The second main component is a relationship: association among two or more entities. A relationship is usually depicted as a diamond in our E-R model. Again, the attributes associated with the relationship are depicted as ovals. For example, the following figure shows \"works in\" as a relationship between the entities employee and department. The \"works in\" relationship has the attribute \"since.\"","metadata":{"source":"https://cs186berkeley.net/notes/note13/"}},{"pageContent":"  Solutions  ","metadata":{"source":"https://cs186berkeley.net/notes/note13/"}},{"pageContent":"  Past Exam Problems  Note that the same entity set can participate in different relationship sets, or in different “roles” in the same relationship set.","metadata":{"source":"https://cs186berkeley.net/notes/note13/"}},{"pageContent":"  Motivation  In prior modules we discussed the ACID properties of transactions. In this note we will discuss how to make our database resilient to failures. The two ACID properties that we will learn how to enforce in this note are:","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Force/No Force  Durability: If a transaction finishes (commits), we will never lose the result of the transaction.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Steal/No-Steal  Atomicity: Either all or none of the operations in the transaction will persist. This means that we will never leave the database in an intermediate state.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Steal, No-Force  We can take a look at these two properties in the example of swapping a class in CalCentral:","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Write Ahead Logging  Durability: Once we say that we have made the change, the student should not suddenly find themselves in old section later on!","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  WAL Implementation  Atomicity: There are two operations: dropping your old class and adding the new one. If the database crashes before it can add your new class, you do not actually want to be dropped out of your old class.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Inequality Exercise  Durability can be a very simple property to ensure if we use a force policy. The force policy states when a transaction finishes, force all modified data pages to disk before the transaction commits. This would ensure durability because disk is persistent1; in other words, once a page makes it to disk it is saved permanently. The downside of this approach is performance. We end up doing a lot of unnecessary writes. The policy we would prefer is no force which says to only write back to disk when the page needs to be evicted from the buffer pool. While this helps reduce unnecessary writes, it complicates durability because if the database crashes after the transaction commits, some pages may not have been written to disk and are consequently lost from memory, since memory is volatile. To address this problem, we will redo certain operations during recovery.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Aborting a Transaction  Similarly, it would be easy to ensure atomicity with a no-steal policy. The no-steal policy states that pages cannot be evicted from memory (and thus written to disk) until the transaction commits. This ensures that we do not leave the database in an intermediate state because if the transaction does not finish, then none of its changes are actually written to disk and saved. The problem with this policy is that it handcuffs how we can use memory. We have to keep every modified page in memory until a transaction completes. We would much rather have a steal policy, which allows modified pages to be written to disk before a transaction finishes. This will complicate enforcing atomicity, but we will fix this problem by undoing bad operations during recovery.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Recovery Data Structures  To review, we chose to use two policies (steal, no force) that make it difficult to guarantee atomicity and durability, but get us the best performance. The rest of this note will cover how to ensure atomicity and durability while using a steal, no force policy.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  More Inequality Questions  To solve these complications we will use logging. A log is a sequence of log records that describe the operations that the database has done.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Undo Logging  Each write operation (SQL insert/delete/update) will get its own log UPDATE record.  An UPDATE log record looks like this:","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Redo Logging  The fields are:","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  ARIES Recovery Algorithm  XID: transaction ID - tells us which transaction did this operation","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Analysis Phase  pageID: what page has been modified","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Analysis Phase Example  offset: where on the page the data started changing (typically in bytes)","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Redo Phase  length: how much data was changed (typically in bytes)","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Redo Example  old_data: what the data was originally (used for undo operations)","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Undo Phase  new_data: what the data has been updated to (used for redo operations)","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Undo Example  There are a few other record types we will use in our log. We will add fields to these log records throughout the note as the need for these fields becomes apparent.","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Conclusion  COMMIT: signifies that a transaction is starting the commit process","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Practice Questions  ABORT: signifies that a transaction is starting the aborting process","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Solutions  END: signifies that a transaction is finished (usually means that it finsihed committing or aborting)","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Appendix 1: Undo Details  Just like regular data pages, log pages need to be operated on in memory, but need to be written to disk to be stored permanently. Write Ahead Logging (WAL) imposes requirements for when we must write the logs to disk. In short, write ahead logging is a policy where log records describing an action such as modifying a data page or committing a transaction are flushed to disk before the actual action is flushed to disk or occurs, respectively. The two rules are as follows:","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Appendix 2: LSN list  Log records must be written to disk before the corresponding data page gets written to disk. This is how we will achieve atomicity. The intuition for this is that if a data page is written first and then the database crashes we have no way of undoing the operation because we don’t know what operation happened!","metadata":{"source":"https://cs186berkeley.net/notes/note14/"}},{"pageContent":"  Introduction  Up until now, we have assumed that our database is running on a single computer. For modern applications that deal with millions of requests over terabytes of data it would be impossible for one computer to quickly respond to all of those requests. We need to figure out how to run our database on multiple computers. We will call this parallel query processing because a query will be run on multiple machines in parallel.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Architectures  How are these machines connected together? The most straightforward option would probably be to have every CPU share memory and disk. This is called shared memory.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Types of Parallelism  ","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Partitioning  Another option is for each CPU to have its own memory, but all of them share the same disk. This is called shared disk.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Network Cost  .","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Partitioning Practice Questions  These architectures are easy to reason about, but sharing resources holds back the system. It is possible to achieve a much higher level of parallelism if all the machines have their own disk and memory because they do not need to wait for the resource to become available. This architecture is called shared nothing and will be the architecture we use throughout the rest of the note.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Partitioning Practice Solutions  ","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Sorting  In shared nothing, the machines communicate with each other solely through the network by passing messages to each other.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Hashing  The focus of this note will be on intra-query parallelism. Intra-query parallelism attempts to make one query run as fast as possible by spreading the work over multiple computers. The other major type of parallelism is inter-query parallelism which gives each machine different queries to work on so that the system can achieve a high throughput and complete as many queries as possible. This may sound simple, but doing it correctly is actually quite difficult and we will address how to do it when we get to the module on concurrency.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Sort Merge Join  We can further divide Intra-query parallelism into two classes: intra-operator and inter-operator. Intra-operator is making one operator run as quickly as possible. An example of intra-operator parallelism is dividing up the data onto several machines and having them sort the data in parallel. This parallelism makes sorting (one operation) as fast as possible. Inter-operator parallelism is making a query run as fast as possible by running the operators in parallel. For example, imagine our query plan looks like this:","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Grace Hash Join  ","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Broadcast Join  One machine can work on sorting R and another machine can sort S at the same time. In inter-operator parallelism, we parallelize the entire query, not individual operators.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Symmetric Hash Join  The last distinction we will make is between the different forms of inter-operator parallelism. The first type is pipeline parallelism. In pipeline parallelism records are passed to the parent operator as soon as they are done. The parent operator can work on a record that its child has already processed while the child operator is working on a different record. As an example, consider the query plan:","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Hierarchical Aggregation  ","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Algorithms Practice Questions  In pipeline parallelism the project and filter can run at the same time because as soon as filter finishes a record, project can operate on it while filter picks up a new record to operate on. The other type of inter-operator parallelism is bushy tree parallelism in which different branches of the tree are run in parallel. The example in section 3.1 where we sort the two files independently is an example of bushy tree parallelism. This is another example of bushy tree parallelism:","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Parallel Algorithms Practice Solutions  ","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Past Exam Problems  In bushy tree parallelism, the left branch and right branch can execute at the same time. For instance, scanning R, S, T, U can all happen at the same time. After that, joining R with S and joining T with U can happen in parallel as well.","metadata":{"source":"https://cs186berkeley.net/notes/note15/"}},{"pageContent":"  Introduction  For much of this semester, we assumed that transactions ran on databases where all of the data existed on one node (machine). This is often the case for databases with lighter workloads, but as demand increases, databases scale out to improve performance by using a Shared Nothing architecture. Each node receives a partition of the data set that is distributed based on a range or hash key and is connected to other nodes through a network. Distributed Transactions are needed for executing queries in distributed databases as a transaction may need to perform reads and writes on data that exist on different nodes.","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Distributed Locking  Since every node contains data that is independent of any other node’s data, every node can maintain its own local lock table. This works if the data fits entirely within that node, such as pages or tuples of data. For coarser grained locks on data that spans multiple nodes, such as a table or database, the coarser grained locks can either be given to all nodes containing a partition of that data or be centralized at a predetermined master node. This design makes locking simple as 2 phase locking is performed at every node using local locks in order to guarantee serializability between different transactions.  When dealing with locking, deadlock is always a possibility. To determine whether deadlock has occurred in a distributed database, the waits-for graphs for each node must be unioned to find cycles as transactions can be blocked by other transactions executing on different nodes. Do this by drawing all waits-for graphs on top of each other. If a cycle is detected in this unioned graph, then there is a deadlock within the distributed system.","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Two Phase Commit (2 PC)   ","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Distributed Recovery (2PC)  With a distributed database where transactions can be running on multiple nodes, how do the various nodes know whether to commit or abort a transaction? In either case, commit or abort, we want to ensure that all nodes reach consensus, which is to say that all nodes agree on one course of action. Consensus is implemented through Two Phase Commit and enforces the property that all nodes maintain the same view of the data. It provides this guarantee by ensuring that a distributed transaction either commits or aborts on all nodes involved. If consensus is not enforced, some nodes may commit the transaction while others abort, causing nodes to have views of data at different points in time.  Every distributed transaction is assigned a coordinator node that is responsible for maintaining consensus among all participant nodes involved in the transaction. When the transaction is ready to commit, the coordinator initiates Two Phase Commit.  2 PC’s first phase is the preparation phase:","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Distributed Recovery (2PC with Presumed Abort)  Coordinator sends prepare message to participants to tell participants to either prepare for commit or abort","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Practice Questions  Participants generate a prepare or abort record and flush record to disk","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Solutions  Participants send yes vote to coordinator if prepare record is flushed or no vote if abort record is flushed","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Past Exam Problems  Coordinator generates a commit record if it receives unanimous yes votes or an abort record otherwise, and flushes the record to disk","metadata":{"source":"https://cs186berkeley.net/notes/note16/"}},{"pageContent":"  Introduction  Most of the semester has focused on the properties and implementation of relational database management systems, but here we will take a detour to explore a relatively new1 class of database that has come to be called NoSQL. NoSQL databases deviate from the set of database design principles that we have studied in order to achieve the high scale and elasticity needed for modern “Web 2.0” applications. In this document we will explore the motivation and history of NoSQL databases, review the design and properties of several types of NoSQL databases, compare the NoSQL data model with the relational data model, and conclude with a description of MongoDB, a popular and representative NoSQL database.","metadata":{"source":"https://cs186berkeley.net/notes/note17/"}},{"pageContent":"  Motivation and History  In order to understand the emergence and properties of NoSQL, we must first describe two broad classes of workloads that can be subjected to a database.","metadata":{"source":"https://cs186berkeley.net/notes/note17/"}},{"pageContent":"  Methods for Scaling Databases  Online Transaction Processing (OLTP) is a class of workloads characterized by high numbers of transactions executed by large numbers of users. This kind of workload is common to “frontend” applications such as social networks and online stores. Queries are typically simple lookups (e.g., “find user by ID”, “get items in shopping cart”) and rarely include joins. OLTP workloads also involve high numbers of updates (e.g., “post tweet”, “add item to shopping cart”). Because of the need for consistency in these business-critical workloads, the queries, inserts and updates are performed as transactions.","metadata":{"source":"https://cs186berkeley.net/notes/note17/"}},{"pageContent":"  NoSQL Data Models  Online Analytical Processing is a class of read-only workloads characterized by queries that typically touch a large amount of data. OLAP queries typically involve large numbers of joins and aggregations in order to support decision making (e.g., “sum revenues by store, region, clerk, product, date”).","metadata":{"source":"https://cs186berkeley.net/notes/note17/"}},{"pageContent":"  Document vs Relational Data Models  In many cases, OLTP and OLAP workloads are served by separate databases. Data must be migrated from OLTP systems to OLAP systems every so often via a process called extract-transform-load (ETL).","metadata":{"source":"https://cs186berkeley.net/notes/note17/"}},{"pageContent":"  Introduction  In previous modules, we learned how to parallelize relational database systems which is useful for optimizing data processing but only works well up to a certain number of machines. Difficulties and headaches come when we start thinking about scaling the relational model on databases split across hundreds or thousands of machines. This became a problem when more and more people, especially every day consumers, started to interact with databases when the Internet exploded around the turn of the 21st century. Database thinking, models, and technologies needed to catch up to meet the demand. This note will focus on two recent advances in parallel database technologies - MapReduce and Spark - that have enabled the massive scalability of modern data processing.","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}},{"pageContent":"  MapReduce  Engineers at Google in the early 2000s needed a way to efficiently manage and process the large amounts of data (at petabyte scale and beyond) that was being generated, stored, and indexed by the company. First off, they needed a way to store files across hundreds and eventually thousands of machines since only a few would definitely not be enough. To do this, they designed a file system in which large files (TBs, PBs) are partitioned into smaller files called chunks (usually 64MB) and then distributed and replicated several times on different nodes for fault tolerance. This is called a distributed file system (DFS) which has had countless implementations since then from Google’s proprietary GFS to Hadoop’s open source HDFS. Next, they needed a way to efficiently process the large amount of data stored on a DFS. To do this, they built MapReduce which is a high-level programming model and implementation for large-scale parallel data processing. Its name is derived from the two main and separate phases of the process: Map and Reduce. From a high level, we can describe the Map phase as applying a function in parallel to every element of a set of data and the Reduce phase as combining the results of the Map phase into the desired data output. The magic of this paradigm in processing data at scale is it automatically handles the details of issuing and managing tasks in parallel across multiple machines. A user only has to define the Map and Reduce tasks and MapReduce takes care of the rest, similar to how SQL creates an execution plan from a query.","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}},{"pageContent":"  Spark  The data model in MapReduce works on files called bags, which contain pairs. A MapReduce program takes in an input of a bag of pairs and outputs a bag of pairs ( is optional). The user must provide two stateless functions – and – to define how the input pairs will be transformed into the output pair domain. The first part of MapReduce – the Map phase – applies a user-provided Map function in parallel to an input of pairs and outputs a bag of pairs. The types and values of , , , and are independent of each other. The pairs serve as intermediate tuples in the MapReduce process.","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}},{"pageContent":"  Conclusion  The second part of MapReduce – the Reduce phase – groups all intermediate pairs outputted by the Map phase with the same and processes each group into a single bag of output values defined by a user-provided Reduce function. This function is also run in parallel with a machine or worker thread processing a single group of pairs with the same . The pairs in this function serve as the output tuples in the MapReduce process.","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}},{"pageContent":"  Practice Questions  Let’s start with an example of counting the number of occurrences of each word in a large collection of documents. Each document is a bag of key-value pairs with key being the document and value being the entire set of words in that document. We define the following map function to emit an intermediate pair for every word that appears in a document. Note that this function is stateless so we can run it in parallel.","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}},{"pageContent":"  Solutions  After applying this map function to all documents, we essentially have a collection of all individual words that appear in all documents with a 1 assigned to each to represent the number of appearances. For example:","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}},{"pageContent":"  Past Exam Problems  MapReduce then groups intermediate pairs with the same word key and for each group, combines their values (in this case all 1s) into an iterator. The following reduce function takes the iterator for each word key and returns the sum over the 1s in the values, resulting in the count of each word across all documents.","metadata":{"source":"https://cs186berkeley.net/notes/note18/"}}]